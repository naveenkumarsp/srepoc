groups:
 - name: Monitoring stack
   rules:
    - alert: PrometheusConfigurationReload
      expr: prometheus_config_last_reload_successful != 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Prometheus configuration reload (instance {{ $labels.instance }})"
        description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: AlertmanagerConfigurationReload
      expr: alertmanager_config_last_reload_successful != 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "AlertManager configuration reload (instance {{ $labels.instance }})"
        description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ExporterDown
      expr: up == 0
      for: 30s
      labels:
        severity: warning
      annotations:
        summary: "Exporter down (instance {{ $labels.instance }})"
        description: "Prometheus exporter down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticHeapUsageTooHigh
      expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 75
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Elastic Heap Usage Too High (instance {{ $labels.instance }})"
        description: "The heap usage is over 75% for 2m\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticHeapUsageWarning
      expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Elastic Heap Usage warning (instance {{ $labels.instance }})"
        description: "The heap usage is over 80% for 2m\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticClusterRed
      expr: elasticsearch_cluster_health_status{color="red"} == 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Elastic Cluster Red (instance {{ $labels.instance }})"
        description: "Elastic Cluster Red status\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticClusterYellow
      expr: elasticsearch_cluster_health_status{color="yellow"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Elastic Cluster Yellow (instance {{ $labels.instance }})"
        description: "Elastic Cluster Yellow status\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfElasticHealthyNodes
      expr: elasticsearch_cluster_health_number_of_nodes < number_of_nodes
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of Elastic Healthy Nodes (instance {{ $labels.instance }})"
        description: "Number Healthy Nodes less then number_of_nodes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfElasticHealthyDataNodes
      expr: elasticsearch_cluster_health_number_of_data_nodes < number_of_data_nodes
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of Elastic Healthy Data Nodes (instance {{ $labels.instance }})"
        description: "Number Healthy Data Nodes less then number_of_data_nodes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfRelocationShards
      expr: elasticsearch_cluster_health_relocating_shards > 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of relocation shards (instance {{ $labels.instance }})"
        description: "Number of relocation shards for 20 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfInitializingShards
      expr: elasticsearch_cluster_health_initializing_shards > 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of initializing shards (instance {{ $labels.instance }})"
        description: "Number of initializing shards for 10 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfUnassignedShards
      expr: elasticsearch_cluster_health_unassigned_shards > 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of unassigned shards (instance {{ $labels.instance }})"
        description: "Number of unassigned shards for 2 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfPendingTasks
      expr: elasticsearch_cluster_health_number_of_pending_tasks > 0
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Number of pending tasks (instance {{ $labels.instance }})"
        description: "Number of pending tasks for 10 min. Cluster works slowly.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticNoNewDocuments
      expr: rate(elasticsearch_indices_docs{es_data_node="true"}[10m]) < 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Elastic no new documents (instance {{ $labels.instance }})"
        description: "No new documents for 10 min!\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: TraefikBackendDown
      expr: count(traefik_backend_server_up) by (backend) == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Traefik backend down (instance {{ $labels.instance }})"
        description: "All Traefik backends are down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: TraefikBackendErrors
      expr: sum(rate(traefik_backend_requests_total{code=~"5.*"}[2m])) by (backend) / sum(rate(traefik_backend_requests_total[2m])) by (backend) > 0.1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Traefik backend errors (instance {{ $labels.instance }})"
        description: "Traefik backend error rate is above 10%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: JvmMemoryFillingUp
      expr: jvm_memory_bytes_used / jvm_memory_bytes_max{area="heap"} > 0.8
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "JVM memory filling up (instance {{ $labels.instance }})"
        description: "JVM memory is filling up (> 80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesMemorypressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kubernetes MemoryPressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesDiskpressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kubernetes DiskPressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesOutofdisk
      expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kubernetes OutOfDisk (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesJobFailed
      expr: kube_job_status_failed > 0
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes Job failed (instance {{ $labels.instance }})"
        description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesCronjobSuspended
      expr: kube_cronjob_spec_suspend != 0
      for: 2m
      labels:
        severity: info
      annotations:
        summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})"
        description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesPersistentvolumeclaimPending
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
        description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: VolumeOutOfDiskSpace
      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 20
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Volume out of disk space (instance {{ $labels.instance }})"
        description: "Volume is almost full (< 20% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: VolumeFullInFourDays
      expr: 100 * (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) < 15 and predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Volume full in four days (instance {{ $labels.instance }})"
        description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: StatefulsetDown
      expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "StatefulSet down (instance {{ $labels.instance }})"
        description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        
 - name: Node exporter stack
   rules:
    - alert: OutOfMemory
      expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Out of memory (instance {{ $labels.instance }})"
        description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualNetworkThroughputIn
      expr: sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual network throughput in (instance {{ $labels.instance }})"
        description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualNetworkThroughputOut
      expr: sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual network throughput out (instance {{ $labels.instance }})"
        description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualDiskReadRate
      expr: sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual disk read rate (instance {{ $labels.instance }})"
        description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualDiskWriteRate
      expr: sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual disk write rate (instance {{ $labels.instance }})"
        description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: OutOfDiskSpace
      expr: node_filesystem_free_bytes{mountpoint ="/rootfs"} / node_filesystem_size_bytes{mountpoint ="/rootfs"} * 100 < 30
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Out of disk space (instance {{ $labels.instance }})"
        description: "Disk is almost full (< 30% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: OutOfInodes
      expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint ="/rootfs"} * 100 < 10
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Out of inodes (instance {{ $labels.instance }})"
        description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualDiskReadLatency
      expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual disk read latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualDiskWriteLatency
      expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual disk write latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighCpuLoad
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High CPU load (instance {{ $labels.instance }})"
        description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    # 1000 context switches is an arbitrary number.
    # Alert threshold depends on nature of application.
    # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58

    - alert: ContextSwitching
      expr: rate(node_context_switches_total[2m]) > 1000
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Context switching (instance {{ $labels.instance }})"
        description: "Context switching is growing on node (> 1000 / s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SwapIsFillingUp
      expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Swap is filling up (instance {{ $labels.instance }})"
        description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SystemdServiceCrashed
      expr: node_systemd_unit_state{state="failed"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "SystemD service crashed (instance {{ $labels.instance }})"
        description: "SystemD service crashed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ServiceHealthcheckFailed
      expr: consul_catalog_service_node_healthy == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Service healthcheck failed (instance {{ $labels.instance }})"
        description: "Service: `{{ $labels.service_name }}` Healthcheck: `{{ $labels.service_id }}`\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: MissingConsulMasterNode
      expr: consul_raft_peers < number_of_consul_master
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Missing Consul master node (instance {{ $labels.instance }})"
        description: "Numbers of consul raft peers less then expected <https://example.ru/ui/{{ $labels.dc }}/services/consul|Consul masters>\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: InsufficientMembers
      expr: count(etcd_server_id) > (count(etcd_server_id) / 2 - 1)
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Insufficient Members (instance {{ $labels.instance }})"
        description: "Etcd cluster should have an odd number of members\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NoLeader
      expr: etcd_server_has_leader == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "No Leader (instance {{ $labels.instance }})"
        description: "Etcd cluster have no leader\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfLeaderChanges
      expr: increase(etcd_server_leader_changes_seen_total[1h]) > 3
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High number of leader changes (instance {{ $labels.instance }})"
        description: "Etcd leader changed more than 3 times during last hour\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedGrpcRequests
      expr: sum(rate(grpc_server_handled_total{grpc_code!="OK"}[2m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total[2m])) BY (grpc_service, grpc_method) > 0.01
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High number of failed GRPC requests (instance {{ $labels.instance }})"
        description: "More than 1% GRPC request failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedGrpcRequests
      expr: sum(rate(grpc_server_handled_total{grpc_code!="OK"}[2m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total[2m])) BY (grpc_service, grpc_method) > 0.05
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "High number of failed GRPC requests (instance {{ $labels.instance }})"
        description: "More than 5% GRPC request failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: GrpcRequestsSlow
      expr: histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{grpc_type="unary"}[2m])) by (grpc_service, grpc_method, le)) > 0.15
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "GRPC requests slow (instance {{ $labels.instance }})"
        description: "GRPC requests slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedHttpRequests
      expr: sum(rate(etcd_http_failed_total[2m])) BY (method) / sum(rate(etcd_http_received_total[2m])) BY (method) > 0.01
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High number of failed HTTP requests (instance {{ $labels.instance }})"
        description: "More than 1% HTTP failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedHttpRequests
      expr: sum(rate(etcd_http_failed_total[2m])) BY (method) / sum(rate(etcd_http_received_total[2m])) BY (method) > 0.05
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "High number of failed HTTP requests (instance {{ $labels.instance }})"
        description: "More than 5% HTTP failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HttpRequestsSlow
      expr: histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[2m])) > 0.15
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "HTTP requests slow (instance {{ $labels.instance }})"
        description: "HTTP requests slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdMemberCommunicationSlow
      expr: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[2m])) > 0.15
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Etcd member communication slow (instance {{ $labels.instance }})"
        description: "Etcd member communication slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedProposals
      expr: increase(etcd_server_proposals_failed_total[1h]) > 5
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High number of failed proposals (instance {{ $labels.instance }})"
        description: "Etcd server got more than 5 failed proposals past hour\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighFsyncDurations
      expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m])) > 0.5
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High fsync durations (instance {{ $labels.instance }})"
        description: "Etcd WAL fsync duration increasing, 99th percentil is over 0.5s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighCommitDurations
      expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[2m])) > 0.25
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High commit durations (instance {{ $labels.instance }})"
        description: "Etcd commit duration increasing, 99th percentil is over 0.25s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KafkaTopics
      expr: sum(kafka_topic_partition_in_sync_replica) by (topic) < 3
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kafka Topics (instance {{ $labels.instance }})"
        description: "Kafka topic in-sync partition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KafkaConsumersGroup
      expr: sum(kafka_consumergroup_lag) by (consumergroup) > 50
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kafka consumers group (instance {{ $labels.instance }})"
        description: "Kafka consumers group\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ProbeFailed
      expr: probe_success == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Probe failed (instance {{ $labels.instance }})"
        description: "Probe failed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SlowProbe
      expr: avg_over_time(probe_duration_seconds[1m]) > 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Slow probe (instance {{ $labels.instance }})"
        description: "Blackbox probe took more than 1s to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HttpStatusCode
      expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "HTTP Status Code (instance {{ $labels.instance }})"
        description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SslCertificateWillExpireSoon
      expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 30
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "SSL certificate will expire soon (instance {{ $labels.instance }})"
        description: "SSL certificate expires in 30 days\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SslCertificateExpired
      expr: probe_ssl_earliest_cert_expiry - time()  <= 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "SSL certificate expired (instance {{ $labels.instance }})"
        description: "SSL certificate has expired already\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HttpSlowRequests
      expr: avg_over_time(probe_http_duration_seconds[1m]) > 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "HTTP slow requests (instance {{ $labels.instance }})"
        description: "HTTP request took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SlowPing
      expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Slow ping (instance {{ $labels.instance }})"
        description: "Blackbox ping took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: CollectorError
      expr: wmi_exporter_collector_success == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Collector Error (instance {{ $labels.instance }})"
        description: "Collector {{ $labels.collector }} was not successful\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ServiceStatus
      expr: wmi_service_status{status="ok"} != 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Service Status (instance {{ $labels.instance }})"
        description: "Windows Service state is not OK\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: CpuUsage
      expr: 100 - (avg by (instance) (irate(wmi_cpu_time_total{mode="idle"}[2m])) * 100) > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "CPU Usage (instance {{ $labels.instance }})"
        description: "CPU Usage is more than 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: MemoryUsage
      expr: 100*(wmi_os_physical_memory_free_bytes) / wmi_cs_physical_memory_bytes > 75
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Memory Usage (instance {{ $labels.instance }})"
        description: "Memory Usage is more than 75%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: DiskSpaceUsage
      expr: 100.0 - 100 * ((wmi_logical_disk_free_bytes{} / 1024 / 1024 ) / (wmi_logical_disk_size_bytes{}  / 1024 / 1024)) > 80
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Disk Space Usage (instance {{ $labels.instance }})"
        description: "Disk Space on Drive is used more than 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UsedPoolCapacity
      expr: (openebs_used_pool_capacity_percent) > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Used pool capacity (instance {{ $labels.instance }})"
        description: "OpenEBS Pool use more than 80% of his capacity\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: DiskDown
      expr: minio_offline_disks > 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Disk down (instance {{ $labels.instance }})"
        description: "Minio Disk is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

 - name: Black Box stack
   rules:
    - alert: BlackboxSlowProbe
      expr: avg_over_time(probe_duration_seconds[1m]) > 1
      for: 10s 
      labels:
        severity: warning
      annotations:
        summary: "Blackbox slow probe (instance {{ $labels.instance }})"
        description: "Blackbox probe took more than 1s to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: BlackboxProbeHttpFailure
      expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
      for: 10s
      labels:
        severity: error
      annotations:
        summary: "Blackbox probe HTTP failure (instance {{ $labels.instance }})"
        description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: BlackboxSslCertificateWillExpireSoon
      expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 30
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: "Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})"
        description: "SSL certificate expires in 30 days\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: BlackboxSslCertificateWillExpireSoon
      expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 3
      for: 10s
      labels:
        severity: error
      annotations:
        summary: "Blackbox SSL certificate will expire soon (instance {{ $labels.instance }})"
        description: "SSL certificate expires in 3 days\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: BlackboxSslCertificateExpired
      expr: probe_ssl_earliest_cert_expiry - time() <= 0
      for: 10s
      labels:
        severity: error
      annotations:
        summary: "Blackbox SSL certificate expired (instance {{ $labels.instance }})"
        description: "SSL certificate has expired already\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: BlackboxProbeSlowHttp
      expr: avg_over_time(probe_http_duration_seconds[1m]) > 1
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: "Blackbox probe slow HTTP (instance {{ $labels.instance }})"
        description: "HTTP request took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: BlackboxProbeSlowPing
      expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 1
      for: 10s
      labels:
        severity: warning
      annotations:
        summary: "Blackbox probe slow ping (instance {{ $labels.instance }})"
        description: "Blackbox ping took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

 - name: Application Runtime  
   rules:
    - alert: ApacheDown
      expr: apache_up == 0
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Apache down (instance {{ $labels.instance }})"
        description: "Apache down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ApacheWorkersLoad
      expr: (sum by (instance) (apache_workers{state="busy"}) / sum by (instance) (apache_scoreboard) ) * 100 > 80
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Apache workers load (instance {{ $labels.instance }})"
        description: "Apache workers in busy state approach the max workers count 80% workers busy on {{ $labels.instance }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ApacheRestart
      expr: apache_uptime_seconds_total / 60 < 1
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Apache restart (instance {{ $labels.instance }})"
        description: "Apache has just been restarted, less than one minute ago.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NginxHighHttp4xxErrorRate
      expr: sum(rate(nginx_http_requests_total{status=~"^4.."}[1m])) / sum(rate(nginx_http_requests_total[1m])) * 100 > 5
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Nginx high HTTP 4xx error rate (instance {{ $labels.instance }})"
        description: "Too many HTTP requests with status 4xx (> 5%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NginxHighHttp5xxErrorRate
      expr: sum(rate(nginx_http_requests_total{status=~"^5.."}[1m])) / sum(rate(nginx_http_requests_total[1m])) * 100 > 5
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Nginx high HTTP 5xx error rate (instance {{ $labels.instance }})"
        description: "Too many HTTP requests with status 5xx (> 5%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NginxLatencyHigh
      expr: histogram_quantile(0.99, sum(rate(nginx_http_request_duration_seconds_bucket[30m])) by (host, node)) > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Nginx latency high (instance {{ $labels.instance }})"
        description: "Nginx p99 latency is higher than 10 seconds\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyDown
      expr: haproxy_up < 1
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy down (instance {{ $labels.instance }})"
        description: "HAProxy down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyHighHttp4xxErrorRateBackend
      expr: (sum by (backend) (irate(haproxy_server_http_responses_total{code="4xx"}[1m])) / sum by (backend) (irate(haproxy_server_http_responses_total{}[1m])) * 100)> 2
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy high HTTP 4xx error rate backend (instance {{ $labels.instance }})"
        description: "Too many HTTP requests with status 4xx (> 5%) on backend {{ $labels.fqdn }}/{{ $labels.backend }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyHighHttp4xxErrorRateServer
      expr: (sum by (server) (irate(haproxy_server_http_responses_total{code="4xx"}[1m]))) / (sum by (backend) (irate(haproxy_server_http_responses_total{}[1m]))) * 100 > 2
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy high HTTP 4xx error rate server (instance {{ $labels.instance }})"
        description: "Too many HTTP requests with status 4xx (> 5%) on server {{ $labels.server }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyHighHttp5xxErrorRateServer
      expr: (sum by (server) (irate(haproxy_server_http_responses_total{code="5xx"}[1m])) / sum by (backend) (irate(haproxy_server_http_responses_total{}[1m])) * 100) > 2
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy high HTTP 5xx error rate server (instance {{ $labels.instance }})"
        description: "Too many HTTP requests with status 5xx (> 5%) on server {{ $labels.server }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyBackendConnectionErrors
      expr: sum by (backend) (rate(haproxy_backend_connection_errors_total[1m]) * 100) > 2
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy backend connection errors (instance {{ $labels.instance }})"
        description: "Too many connection errors to {{ $labels.fqdn }}/{{ $labels.backend }} backend (> 5%). Request throughput may be to high.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyServerResponseErrors
      expr: sum by (server) (rate(haproxy_server_response_errors_total[1m])) * 100 > 2
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy server response errors (instance {{ $labels.instance }})"
        description: "Too many response errors to {{ $labels.server }} server (> 5%).\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyServerConnectionErrors
      expr: sum by (server) (rate(haproxy_server_connection_errors_total[1m]) * 100) > 2
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy server connection errors (instance {{ $labels.instance }})"
        description: "Too many connection errors to {{ $labels.server }} server (> 5%). Request throughput may be to high.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyPendingRequests
      expr: sum by (backend) (haproxy_backend_current_queue) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy pending requests (instance {{ $labels.instance }})"
        description: "Some HAProxy requests are pending on {{ $labels.fqdn }}/{{ $labels.backend }} backend\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyHttpSlowingDown
      expr: avg by (backend) (haproxy_backend_http_total_time_average_seconds) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy HTTP slowing down (instance {{ $labels.instance }})"
        description: "Average request time is increasing\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyRetryHigh
      expr: rate(haproxy_backend_retry_warnings_total[1m]) > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy retry high (instance {{ $labels.instance }})"
        description: "High rate of retry on {{ $labels.fqdn }}/{{ $labels.backend }} backend\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyBackendDown
      expr: haproxy_backend_up < 1
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy backend down (instance {{ $labels.instance }})"
        description: "HAProxy backend is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyServerDown
      expr: haproxy_server_up < 1
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "HAProxy server down (instance {{ $labels.instance }})"
        description: "HAProxy server is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HaproxyServerHealthcheckFailure
      expr: increase(haproxy_server_check_failures_total[1m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "HAProxy server healthcheck failure (instance {{ $labels.instance }})"
        description: "Some server healthcheck are failing on {{ $labels.server }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: JvmMemoryFillingUp
      expr: jvm_memory_bytes_used / jvm_memory_bytes_max{area="heap"} > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "JVM memory filling up (instance {{ $labels.instance }})"
        description: "JVM memory is filling up (> 80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

 - name: PostgreSQL  
   rules:
    - alert: PostgresqlDown
      expr: pg_up == 0
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql down (instance {{ $labels.instance }})"
        description: "Postgresql instance is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlRestarted
      expr: time() - pg_postmaster_start_time_seconds < 60
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql restarted (instance {{ $labels.instance }})"
        description: "Postgresql restarted\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlExporterError
      expr: pg_exporter_last_scrape_error > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql exporter error (instance {{ $labels.instance }})"
        description: "Postgresql exporter is showing errors. A query may be buggy in query.yaml\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
 
    - alert: PostgresqlTableNotVaccumed
      expr: time() - pg_stat_user_tables_last_autovacuum > 60 * 60 * 24
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql table not vaccumed (instance {{ $labels.instance }})"
        description: "Table has not been vaccum for 24 hours\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlTableNotAnalyzed
      expr: time() - pg_stat_user_tables_last_autoanalyze > 60 * 60 * 24
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql table not analyzed (instance {{ $labels.instance }})"
        description: "Table has not been analyzed for 24 hours\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlTooManyConnections
      expr: sum by (datname) (pg_stat_activity_count{datname!~"template.*|postgres"}) > pg_settings_max_connections * 0.9
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql too many connections (instance {{ $labels.instance }})"
        description: "PostgreSQL instance has too many connections\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlNotEnoughConnections
      expr: sum by (datname) (pg_stat_activity_count{datname!~"template.*|postgres"}) < 5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql not enough connections (instance {{ $labels.instance }})"
        description: "PostgreSQL instance should have more connections (> 5)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlDeadLocks
      expr: rate(pg_stat_database_deadlocks{datname!~"template.*|postgres"}[1m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql dead locks (instance {{ $labels.instance }})"
        description: "PostgreSQL has dead-locks\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlSlowQueries
      expr: rate(pg_slow_queries[1m]) * 60 > 10
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql slow queries (instance {{ $labels.instance }})"
        description: "PostgreSQL executes slow queries (> 1min)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlHighRollbackRate
      expr: rate(pg_stat_database_xact_rollback{datname!~"template.*"}[3m]) / rate(pg_stat_database_xact_commit{datname!~"template.*"}[3m]) > 0.02
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql high rollback rate (instance {{ $labels.instance }})"
        description: "Ratio of transactions being aborted compared to committed is > 2 %\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlCommitRateLow
      expr: rate(pg_stat_database_xact_commit[1m]) < 10
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql commit rate low (instance {{ $labels.instance }})"
        description: "Postgres seems to be processing very few transactions\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlLowXidConsumption
      expr: rate(pg_txid_current[1m]) < 5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql low XID consumption (instance {{ $labels.instance }})"
        description: "Postgresql seems to be consuming transaction IDs very slowly\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqllowXlogConsumption
      expr: rate(pg_xlog_position_bytes[1m]) < 100
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresqllow XLOG consumption (instance {{ $labels.instance }})"
        description: "Postgres seems to be consuming XLOG very slowly\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlWaleReplicationStopped
      expr: rate(pg_xlog_position_bytes[1m]) == 0
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql WALE replication stopped (instance {{ $labels.instance }})"
        description: "WAL-E replication seems to be stopped\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlHighRateStatementTimeout
      expr: rate(postgresql_errors_total{type="statement_timeout"}[5m]) > 3
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql high rate statement timeout (instance {{ $labels.instance }})"
        description: "Postgres transactions showing high rate of statement timeouts\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlHighRateDeadlock
      expr: rate(postgresql_errors_total{type="deadlock_detected"}[1m]) * 60 > 1
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql high rate deadlock (instance {{ $labels.instance }})"
        description: "Postgres detected deadlocks\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
 
    - alert: PostgresqlUnusedReplicationSlot
      expr: pg_replication_slots_active == 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql unused replication slot (instance {{ $labels.instance }})"
        description: "Unused Replication Slots\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlTooManyDeadTuples
      expr: ((pg_stat_user_tables_n_dead_tup > 10000) / (pg_stat_user_tables_n_live_tup + pg_stat_user_tables_n_dead_tup)) >= 0.1 unless ON(instance) (pg_replication_is_replica == 1)
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql too many dead tuples (instance {{ $labels.instance }})"
        description: "PostgreSQL dead tuples is too large\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlSplitBrain
      expr: count(pg_replication_is_replica == 0) != 1
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql split brain (instance {{ $labels.instance }})"
        description: "Split Brain, too many primary Postgresql databases in read-write mode\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlPromotedNode
      expr: pg_replication_is_replica and changes(pg_replication_is_replica[1m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Postgresql promoted node (instance {{ $labels.instance }})"
        description: "Postgresql standby server has been promoted as primary node\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlSslCompressionActive
      expr: sum(pg_stat_ssl_compression) > 0
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql SSL compression active (instance {{ $labels.instance }})"
        description: "Database connections with SSL compression enabled. This may add significant jitter in replication delay. Replicas should turn off SSL compression via `sslcompression=0` in `recovery.conf`.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: PostgresqlTooManyLocksAcquired
      expr: ((sum (pg_locks_count)) / (pg_settings_max_locks_per_transaction * pg_settings_max_connections)) > 0.20
      for: 5m
      labels:
        severity: error
      annotations:
        summary: "Postgresql too many locks acquired (instance {{ $labels.instance }})"
        description: "Too many locks acquired on the database. If this alert happens frequently, we may need to increase the postgres setting max_locks_per_transaction.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
