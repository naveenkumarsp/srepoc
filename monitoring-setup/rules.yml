groups:
 - name: Monitoring stack
   rules:
    - alert: PrometheusConfigurationReload
      expr: prometheus_config_last_reload_successful != 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Prometheus configuration reload (instance {{ $labels.instance }})"
        description: "Prometheus configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: AlertmanagerConfigurationReload
      expr: alertmanager_config_last_reload_successful != 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "AlertManager configuration reload (instance {{ $labels.instance }})"
        description: "AlertManager configuration reload error\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ExporterDown
      expr: up == 0
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Exporter down (instance {{ $labels.instance }})"
        description: "Prometheus exporter down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticHeapUsageTooHigh
      expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 75
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Elastic Heap Usage Too High (instance {{ $labels.instance }})"
        description: "The heap usage is over 75% for 2m\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticHeapUsageWarning
      expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Elastic Heap Usage warning (instance {{ $labels.instance }})"
        description: "The heap usage is over 80% for 2m\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticClusterRed
      expr: elasticsearch_cluster_health_status{color="red"} == 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Elastic Cluster Red (instance {{ $labels.instance }})"
        description: "Elastic Cluster Red status\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticClusterYellow
      expr: elasticsearch_cluster_health_status{color="yellow"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Elastic Cluster Yellow (instance {{ $labels.instance }})"
        description: "Elastic Cluster Yellow status\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfElasticHealthyNodes
      expr: elasticsearch_cluster_health_number_of_nodes < number_of_nodes
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of Elastic Healthy Nodes (instance {{ $labels.instance }})"
        description: "Number Healthy Nodes less then number_of_nodes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfElasticHealthyDataNodes
      expr: elasticsearch_cluster_health_number_of_data_nodes < number_of_data_nodes
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of Elastic Healthy Data Nodes (instance {{ $labels.instance }})"
        description: "Number Healthy Data Nodes less then number_of_data_nodes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfRelocationShards
      expr: elasticsearch_cluster_health_relocating_shards > 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of relocation shards (instance {{ $labels.instance }})"
        description: "Number of relocation shards for 20 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfInitializingShards
      expr: elasticsearch_cluster_health_initializing_shards > 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of initializing shards (instance {{ $labels.instance }})"
        description: "Number of initializing shards for 10 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfUnassignedShards
      expr: elasticsearch_cluster_health_unassigned_shards > 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Number of unassigned shards (instance {{ $labels.instance }})"
        description: "Number of unassigned shards for 2 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NumberOfPendingTasks
      expr: elasticsearch_cluster_health_number_of_pending_tasks > 0
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Number of pending tasks (instance {{ $labels.instance }})"
        description: "Number of pending tasks for 10 min. Cluster works slowly.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ElasticNoNewDocuments
      expr: rate(elasticsearch_indices_docs{es_data_node="true"}[10m]) < 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Elastic no new documents (instance {{ $labels.instance }})"
        description: "No new documents for 10 min!\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: TraefikBackendDown
      expr: count(traefik_backend_server_up) by (backend) == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Traefik backend down (instance {{ $labels.instance }})"
        description: "All Traefik backends are down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: TraefikBackendErrors
      expr: sum(rate(traefik_backend_requests_total{code=~"5.*"}[2m])) by (backend) / sum(rate(traefik_backend_requests_total[2m])) by (backend) > 0.1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Traefik backend errors (instance {{ $labels.instance }})"
        description: "Traefik backend error rate is above 10%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: JvmMemoryFillingUp
      expr: jvm_memory_bytes_used / jvm_memory_bytes_max{area="heap"} > 0.8
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "JVM memory filling up (instance {{ $labels.instance }})"
        description: "JVM memory is filling up (> 80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesMemorypressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kubernetes MemoryPressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesDiskpressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kubernetes DiskPressure (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesOutofdisk
      expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kubernetes OutOfDisk (instance {{ $labels.instance }})"
        description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesJobFailed
      expr: kube_job_status_failed > 0
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes Job failed (instance {{ $labels.instance }})"
        description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesCronjobSuspended
      expr: kube_cronjob_spec_suspend != 0
      for: 2m
      labels:
        severity: info
      annotations:
        summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})"
        description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KubernetesPersistentvolumeclaimPending
      expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
        description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: VolumeOutOfDiskSpace
      expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 20
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Volume out of disk space (instance {{ $labels.instance }})"
        description: "Volume is almost full (< 20% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: VolumeFullInFourDays
      expr: 100 * (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes) < 15 and predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) < 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Volume full in four days (instance {{ $labels.instance }})"
        description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: StatefulsetDown
      expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "StatefulSet down (instance {{ $labels.instance }})"
        description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
        
 - name: Application stack
   rules:
    - alert: OutOfMemory
      expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Out of memory (instance {{ $labels.instance }})"
        description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualNetworkThroughputIn
      expr: sum by (instance) (irate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual network throughput in (instance {{ $labels.instance }})"
        description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualNetworkThroughputOut
      expr: sum by (instance) (irate(node_network_transmit_bytes_total[2m])) / 1024 / 1024 > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual network throughput out (instance {{ $labels.instance }})"
        description: "Host network interfaces are probably sending too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualDiskReadRate
      expr: sum by (instance) (irate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual disk read rate (instance {{ $labels.instance }})"
        description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualDiskWriteRate
      expr: sum by (instance) (irate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual disk write rate (instance {{ $labels.instance }})"
        description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: OutOfDiskSpace
      expr: node_filesystem_free_bytes{mountpoint ="/rootfs"} / node_filesystem_size_bytes{mountpoint ="/rootfs"} * 100 < 30
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Out of disk space (instance {{ $labels.instance }})"
        description: "Disk is almost full (< 30% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: OutOfInodes
      expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint ="/rootfs"} * 100 < 10
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Out of inodes (instance {{ $labels.instance }})"
        description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualDiskReadLatency
      expr: rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual disk read latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UnusualDiskWriteLatency
      expr: rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 100
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Unusual disk write latency (instance {{ $labels.instance }})"
        description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighCpuLoad
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High CPU load (instance {{ $labels.instance }})"
        description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    # 1000 context switches is an arbitrary number.
    # Alert threshold depends on nature of application.
    # Please read: https://github.com/samber/awesome-prometheus-alerts/issues/58

    - alert: ContextSwitching
      expr: rate(node_context_switches_total[2m]) > 1000
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Context switching (instance {{ $labels.instance }})"
        description: "Context switching is growing on node (> 1000 / s)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SwapIsFillingUp
      expr: (1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Swap is filling up (instance {{ $labels.instance }})"
        description: "Swap is filling up (>80%)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SystemdServiceCrashed
      expr: node_systemd_unit_state{state="failed"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "SystemD service crashed (instance {{ $labels.instance }})"
        description: "SystemD service crashed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ServiceHealthcheckFailed
      expr: consul_catalog_service_node_healthy == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Service healthcheck failed (instance {{ $labels.instance }})"
        description: "Service: `{{ $labels.service_name }}` Healthcheck: `{{ $labels.service_id }}`\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: MissingConsulMasterNode
      expr: consul_raft_peers < number_of_consul_master
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Missing Consul master node (instance {{ $labels.instance }})"
        description: "Numbers of consul raft peers less then expected <https://example.ru/ui/{{ $labels.dc }}/services/consul|Consul masters>\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: InsufficientMembers
      expr: count(etcd_server_id) > (count(etcd_server_id) / 2 - 1)
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Insufficient Members (instance {{ $labels.instance }})"
        description: "Etcd cluster should have an odd number of members\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: NoLeader
      expr: etcd_server_has_leader == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "No Leader (instance {{ $labels.instance }})"
        description: "Etcd cluster have no leader\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfLeaderChanges
      expr: increase(etcd_server_leader_changes_seen_total[1h]) > 3
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High number of leader changes (instance {{ $labels.instance }})"
        description: "Etcd leader changed more than 3 times during last hour\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedGrpcRequests
      expr: sum(rate(grpc_server_handled_total{grpc_code!="OK"}[2m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total[2m])) BY (grpc_service, grpc_method) > 0.01
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High number of failed GRPC requests (instance {{ $labels.instance }})"
        description: "More than 1% GRPC request failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedGrpcRequests
      expr: sum(rate(grpc_server_handled_total{grpc_code!="OK"}[2m])) BY (grpc_service, grpc_method) / sum(rate(grpc_server_handled_total[2m])) BY (grpc_service, grpc_method) > 0.05
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "High number of failed GRPC requests (instance {{ $labels.instance }})"
        description: "More than 5% GRPC request failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: GrpcRequestsSlow
      expr: histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{grpc_type="unary"}[2m])) by (grpc_service, grpc_method, le)) > 0.15
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "GRPC requests slow (instance {{ $labels.instance }})"
        description: "GRPC requests slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedHttpRequests
      expr: sum(rate(etcd_http_failed_total[2m])) BY (method) / sum(rate(etcd_http_received_total[2m])) BY (method) > 0.01
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High number of failed HTTP requests (instance {{ $labels.instance }})"
        description: "More than 1% HTTP failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedHttpRequests
      expr: sum(rate(etcd_http_failed_total[2m])) BY (method) / sum(rate(etcd_http_received_total[2m])) BY (method) > 0.05
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "High number of failed HTTP requests (instance {{ $labels.instance }})"
        description: "More than 5% HTTP failure detected in Etcd for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HttpRequestsSlow
      expr: histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[2m])) > 0.15
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "HTTP requests slow (instance {{ $labels.instance }})"
        description: "HTTP requests slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: EtcdMemberCommunicationSlow
      expr: histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket[2m])) > 0.15
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Etcd member communication slow (instance {{ $labels.instance }})"
        description: "Etcd member communication slowing down, 99th percentil is over 0.15s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighNumberOfFailedProposals
      expr: increase(etcd_server_proposals_failed_total[1h]) > 5
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High number of failed proposals (instance {{ $labels.instance }})"
        description: "Etcd server got more than 5 failed proposals past hour\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighFsyncDurations
      expr: histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket[2m])) > 0.5
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High fsync durations (instance {{ $labels.instance }})"
        description: "Etcd WAL fsync duration increasing, 99th percentil is over 0.5s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HighCommitDurations
      expr: histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket[2m])) > 0.25
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High commit durations (instance {{ $labels.instance }})"
        description: "Etcd commit duration increasing, 99th percentil is over 0.25s for 5 minutes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KafkaTopics
      expr: sum(kafka_topic_partition_in_sync_replica) by (topic) < 3
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kafka Topics (instance {{ $labels.instance }})"
        description: "Kafka topic in-sync partition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: KafkaConsumersGroup
      expr: sum(kafka_consumergroup_lag) by (consumergroup) > 50
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Kafka consumers group (instance {{ $labels.instance }})"
        description: "Kafka consumers group\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ProbeFailed
      expr: probe_success == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Probe failed (instance {{ $labels.instance }})"
        description: "Probe failed\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SlowProbe
      expr: avg_over_time(probe_duration_seconds[1m]) > 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Slow probe (instance {{ $labels.instance }})"
        description: "Blackbox probe took more than 1s to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HttpStatusCode
      expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "HTTP Status Code (instance {{ $labels.instance }})"
        description: "HTTP status code is not 200-399\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SslCertificateWillExpireSoon
      expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 30
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "SSL certificate will expire soon (instance {{ $labels.instance }})"
        description: "SSL certificate expires in 30 days\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SslCertificateExpired
      expr: probe_ssl_earliest_cert_expiry - time()  <= 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "SSL certificate expired (instance {{ $labels.instance }})"
        description: "SSL certificate has expired already\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: HttpSlowRequests
      expr: avg_over_time(probe_http_duration_seconds[1m]) > 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "HTTP slow requests (instance {{ $labels.instance }})"
        description: "HTTP request took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: SlowPing
      expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Slow ping (instance {{ $labels.instance }})"
        description: "Blackbox ping took more than 1s\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: CollectorError
      expr: wmi_exporter_collector_success == 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Collector Error (instance {{ $labels.instance }})"
        description: "Collector {{ $labels.collector }} was not successful\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: ServiceStatus
      expr: wmi_service_status{status="ok"} != 1
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Service Status (instance {{ $labels.instance }})"
        description: "Windows Service state is not OK\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: CpuUsage
      expr: 100 - (avg by (instance) (irate(wmi_cpu_time_total{mode="idle"}[2m])) * 100) > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "CPU Usage (instance {{ $labels.instance }})"
        description: "CPU Usage is more than 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: MemoryUsage
      expr: 100*(wmi_os_physical_memory_free_bytes) / wmi_cs_physical_memory_bytes > 75
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Memory Usage (instance {{ $labels.instance }})"
        description: "Memory Usage is more than 75%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: DiskSpaceUsage
      expr: 100.0 - 100 * ((wmi_logical_disk_free_bytes{} / 1024 / 1024 ) / (wmi_logical_disk_size_bytes{}  / 1024 / 1024)) > 80
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Disk Space Usage (instance {{ $labels.instance }})"
        description: "Disk Space on Drive is used more than 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: UsedPoolCapacity
      expr: (openebs_used_pool_capacity_percent) > 80
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Used pool capacity (instance {{ $labels.instance }})"
        description: "OpenEBS Pool use more than 80% of his capacity\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

    - alert: DiskDown
      expr: minio_offline_disks > 0
      for: 2m
      labels:
        severity: error
      annotations:
        summary: "Disk down (instance {{ $labels.instance }})"
        description: "Minio Disk is down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

